{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hard Margin SVM with `cvxopt` Library\n",
    "\n",
    "In this section, we implement the Hard Margin SVM using the `cvxopt` library for convex optimization. We also incorporate multiple kernel functions to explore different decision boundaries. To achieve this, we optimize the following dual form of the SVM:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\; W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "$$\n",
    "\\alpha_i \\geq 0, \\quad i = 1, \\dots, m\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\sum_{i=1}^{m} \\alpha_i y^{(i)} = 0\n",
    "$$\n",
    "\n",
    "This optimization problem is quadratic and subject to linear constraints.\n",
    "\n",
    "### Standard Form for `cvxopt.solvers.qp`\n",
    "\n",
    "The `cvxopt.solvers.qp` function solves problems in the following standard form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min \\frac{1}{2} x^T P x + q^T x \\\\\n",
    "\\text{subject to:} \\quad & G x \\leq h \\\\\n",
    "& A x = b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{where:} \\\\\n",
    "&x \\text{ is the vector of variables we are solving for (in our case, the vector of Lagrange multipliers } \\alpha\\text{)} \\\\\n",
    "&P \\text{ is a matrix representing the quadratic coefficients of the objective function} \\\\\n",
    "&q \\text{ is a vector representing the linear coefficients of the objective function} \\\\\n",
    "&G \\text{ and } h \\text{ represent inequality constraints} \\\\\n",
    "&A \\text{ and } b \\text{ represent equality constraints}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "### Mapping the SVM Dual Problem to the QP Form\n",
    "\n",
    "1. **Variables**:\n",
    "   - Here, $$ x = \\alpha $$, the vector of Lagrange multipliers.\n",
    "\n",
    "2. **Objective Function**:\n",
    "   - The dual objective function can be written as:\n",
    "     $$\n",
    "     W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j K_{ij}\n",
    "     $$\n",
    "     where $$ K_{ij} = \\langle x^{(i)}, x^{(j)} \\rangle $$ is the Gram matrix (or kernel matrix).\n",
    "   \n",
    "   - Since `cvxopt` expects a minimization problem, we need to minimize $$ -W(\\alpha) $$, which becomes:\n",
    "     $$\n",
    "     \\min \\; -\\sum_{i=1}^{m} \\alpha_i + \\frac{1}{2} \\sum_{i,j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j K_{ij}\n",
    "     $$\n",
    "\n",
    "   - This gives:\n",
    "     $$\n",
    "     \\begin{align}\n",
    "     &\\text{Matrix } P: & P_{ij} &= y^{(i)} y^{(j)} K_{ij} \\\\\n",
    "     &\\text{Vector } q: & q_i &= -1 \\quad \\text{(since the linear term is } -\\sum_{i=1}^{m} \\alpha_i \\text{)}\n",
    "     \\end{align}\n",
    "     $$\n",
    "\n",
    "3. **Constraints**:\n",
    "   $$\n",
    "   \\begin{align}\n",
    "   &\\text{Inequality Constraint } (\\alpha_i \\geq 0): & G &= -I \\quad (\\text{negative identity matrix}), \\quad h = 0 \\\\\n",
    "   &\\text{Equality Constraint } \\left(\\sum_{i=1}^{m} \\alpha_i y^{(i)} = 0\\right): & A &= y^T \\quad \\text{(labels vector)}, \\quad b = 0\n",
    "   \\end{align}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Kernel Functions\n",
    "\n",
    "1. **Linear Kernel**  \n",
    "   $$ K(x_1, x_2) = x_1 \\cdot x_2 $$\n",
    "\n",
    "2. **Polynomial Kernel**  \n",
    "   $$ K(x_1, x_2) = (x_1 \\cdot x_2 + \\text{c})^{\\text{degree}} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kernel_matrix(x, y, kwargs):\n",
    "\n",
    "    kernel_type = kwargs['kernel_type']\n",
    "\n",
    "    result = None\n",
    "\n",
    "    ######## YOUR CODE HERE ########\n",
    " \n",
    "    \n",
    "    ################################\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alpha(X, y, K):\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    P_numpy, q_numpy, G_numpy, h_numpy, A_numpy, b_numpy = None, None, None, None, None, None\n",
    "\n",
    "    # Convert inputs to cvxopt format\n",
    "    ######### YOUR CODE HERE #########\n",
    "\n",
    "    ##################################\n",
    "    \n",
    "    # Convert inputs to cvxopt format\n",
    "    P = matrix(P_numpy, tc='d')\n",
    "    q = matrix(q_numpy, tc='d')\n",
    "    G = matrix(G_numpy, tc='d')\n",
    "    h = matrix(h_numpy, tc='d')\n",
    "    A = matrix(A_numpy, tc='d')\n",
    "    b = matrix(b_numpy, tc='d')\n",
    "\n",
    "    # Solve the QP problem to find Lagrange multipliers alpha\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.ravel(solution['x'])\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HardMarginSVM:\n",
    "    def __init__(self, kwargs=None):\n",
    "        self.kwargs = kwargs\n",
    "        self.support_vectors = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def kernel_function(self, x, y):\n",
    "        return compute_kernel_matrix(x, y, self.kwargs)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "\n",
    "        K = self.kernel_function(X, X)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Solve the dual optimization problem to obtain the Lagrange multipliers\n",
    "        self.alpha = find_alpha(X, y, K)\n",
    "\n",
    "        # Select support vectors\n",
    "        support_vector_indices = self.alpha > 1e-8 \n",
    "        self.alpha = self.alpha[support_vector_indices]\n",
    "        self.support_vectors = X[support_vector_indices]\n",
    "        self.support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "        # Compute the bias term b\n",
    "        self.b = None\n",
    "        ######### YOUR CODE HERE #########\n",
    "        \n",
    "\n",
    "        ##################################    \n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        pred = None\n",
    "\n",
    "        ######### YOUR CODE HERE #########\n",
    "\n",
    "\n",
    "        ##################################\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def plot_decision_boundary(self, X, y):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', marker='o', s=30, edgecolors='k', label='Data Points')\n",
    "        plt.scatter(self.support_vectors[:, 0], self.support_vectors[:, 1], s=100, linewidth=1, facecolors='none',\n",
    "                    edgecolors='k', label='Support Vectors')\n",
    "\n",
    "        # Plot decision boundary\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "        grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = self.predict(grid).reshape(xx.shape)\n",
    "\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a linearly separable dataset with support vectors closer to the decision boundary\n",
    "def generate_linear_data():\n",
    "    from sklearn.datasets import make_classification\n",
    "    X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, class_sep=1.0, random_state=42)\n",
    "    y = np.where(y == 0, -1, 1)\n",
    "    return X, y\n",
    "\n",
    "# Generate a non-linearly separable dataset\n",
    "def generate_nonlinear_data():\n",
    "    from sklearn.datasets import make_circles\n",
    "    X, y = make_circles(n_samples=100, factor=0.5, noise=0.1)\n",
    "    y = np.where(y == 0, -1, 1)\n",
    "    return X, y\n",
    "\n",
    "X_linear, y_linear = generate_linear_data()\n",
    "X_no, y_no = generate_nonlinear_data()\n",
    "\n",
    "dict_linear = {'kernel_type': 'linear'}\n",
    "dict_gaussian = {'kernel_type': 'gaussian', 'gamma': 1}\n",
    "dict_polynomial = {'kernel_type': 'polynomial', 'c': 1, 'degree': 3}\n",
    "\n",
    "svm_linear = HardMarginSVM(dict_linear)\n",
    "svm_linear.fit(X_linear, y_linear)\n",
    "print(\"Linear SVM results:\")\n",
    "svm_linear.plot_decision_boundary(X_linear, y_linear)\n",
    "\n",
    "svm_linear.fit(X_no, y_no)\n",
    "print(\"Non-linear SVM results:\")\n",
    "svm_linear.plot_decision_boundary(X_no, y_no)\n",
    "\n",
    "svm_gaussian = HardMarginSVM(dict_polynomial)\n",
    "svm_gaussian.fit(X_no, y_no)\n",
    "print(\"Gaussian SVM results:\")\n",
    "svm_gaussian.plot_decision_boundary(X_no, y_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Soft Margin SVM with `cvxopt` Library\n",
    "\n",
    "For the Soft Margin SVM, we allow for some misclassification errors by introducing a regularization parameter $C$ that controls the trade-off between maximizing the margin and minimizing the classification error. This introduces slack variables, modifying the optimization problem to allow a degree of violation for points that cannot be perfectly separated.\n",
    "\n",
    "The dual formulation for the Soft Margin SVM is as follows:\n",
    "\n",
    "$$\n",
    "\\max_{\\alpha} \\; W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\langle x^{(i)}, x^{(j)} \\rangle\n",
    "$$\n",
    "\n",
    "subject to:\n",
    "$$\n",
    "0 \\leq \\alpha_i \\leq C, \\quad i = 1, \\dots, m\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\sum_{i=1}^{m} \\alpha_i y^{(i)} = 0\n",
    "$$\n",
    "\n",
    "### Standard Form for `cvxopt.solvers.qp`\n",
    "\n",
    "The `cvxopt.solvers.qp` function solves problems in the following standard form:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\min \\frac{1}{2} x^T P x + q^T x \\\\\n",
    "\\text{subject to:} \\quad & G x \\leq h \\\\\n",
    "& A x = b\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x$ is the vector of variables we are solving for (in our case, the vector of Lagrange multipliers $\\alpha$),\n",
    "- $P$ is a matrix representing the quadratic coefficients of the objective function,\n",
    "- $q$ is a vector representing the linear coefficients of the objective function,\n",
    "- $G$ and $h$ represent inequality constraints,\n",
    "- $A$ and $b$ represent equality constraints.\n",
    "\n",
    "### Mapping the Soft Margin SVM Dual Problem to the QP Form\n",
    "\n",
    "1. **Variables**:\n",
    "   - Here, $x = \\alpha$, the vector of Lagrange multipliers.\n",
    "\n",
    "2. **Objective Function**:\n",
    "   - The dual objective function can be written as:\n",
    "     $$\n",
    "     W(\\alpha) = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j K_{ij}\n",
    "     $$\n",
    "     where $K_{ij} = \\langle x^{(i)}, x^{(j)} \\rangle$ is the Gram matrix (or kernel matrix).\n",
    "   \n",
    "   - Since `cvxopt` expects a minimization problem, we need to minimize $-W(\\alpha)$, which becomes:\n",
    "     $$\n",
    "     \\min \\; -\\sum_{i=1}^{m} \\alpha_i + \\frac{1}{2} \\sum_{i,j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j K_{ij}\n",
    "     $$\n",
    "\n",
    "   - This gives:\n",
    "     $$\n",
    "     \\begin{align}\n",
    "     &\\text{Matrix } P: & P_{ij} &= y^{(i)} y^{(j)} K_{ij} \\\\\n",
    "     &\\text{Vector } q: & q_i &= -1 \\quad \\text{(since the linear term is } -\\sum_{i=1}^{m} \\alpha_i \\text{)}\n",
    "     \\end{align}\n",
    "     $$\n",
    "\n",
    "3. **Constraints**:\n",
    "   - Inequality Constraints: We have $0 \\leq \\alpha_i \\leq C$, which translates to:\n",
    "     $$\n",
    "     G = \\begin{bmatrix} -I \\\\ I \\end{bmatrix}, \\quad h = \\begin{bmatrix} 0 \\\\ C \\end{bmatrix}\n",
    "     $$\n",
    "     where $-I$ and $I$ represent the negative and positive identity matrices, respectively, ensuring $\\alpha$ values remain within the interval $[0, C]$.\n",
    "\n",
    "   - Equality Constraint $\\left(\\sum_{i=1}^{m} \\alpha_i y^{(i)} = 0\\right)$:\n",
    "     $$\n",
    "     A = y^T \\quad \\text{(labels vector)}, \\quad b = 0\n",
    "     $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alpha_soft(X, y, K, C):\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    P_numpy, q_numpy, G_numpy, h_numpy, A_numpy, b_numpy = None, None, None, None, None, None\n",
    "\n",
    "    # Convert inputs to cvxopt format\n",
    "    ######### YOUR CODE HERE #########\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    ##################################\n",
    "    \n",
    "    # Convert inputs to cvxopt format\n",
    "    P = matrix(P_numpy, tc='d')\n",
    "    q = matrix(q_numpy, tc='d')\n",
    "    G = matrix(G_numpy, tc='d')\n",
    "    h = matrix(h_numpy, tc='d')\n",
    "    A = matrix(A_numpy, tc='d')\n",
    "    b = matrix(b_numpy, tc='d')\n",
    "\n",
    "    # Solve the QP problem to find Lagrange multipliers alpha\n",
    "    solution = solvers.qp(P, q, G, h, A, b)\n",
    "    alpha = np.ravel(solution['x'])\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SoftMarginSVM:\n",
    "    def __init__(self, kwargs=None):\n",
    "        self.kwargs = kwargs\n",
    "        self.C = kwargs['C']\n",
    "        self.alpha = None\n",
    "        self.support_vectors = None\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def kernel_function(self, x, y):\n",
    "        return compute_kernel_matrix(x, y, self.kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Compute the Kernel matrix\n",
    "        K = self.kernel_function(X, X)\n",
    "\n",
    "        self.alpha = find_alpha_soft(X, y, K, self.C)\n",
    "\n",
    "        # Select support vectors\n",
    "        support_vector_indices = (self.alpha > 1e-8) & (self.alpha < self.kwargs.get('C', np.inf))\n",
    "        self.alpha = self.alpha[support_vector_indices]\n",
    "        self.support_vectors = X[support_vector_indices]\n",
    "        self.support_vector_labels = y[support_vector_indices]\n",
    "\n",
    "        # Compute the bias term b\n",
    "        ######### YOUR CODE HERE #########\n",
    "\n",
    "        \n",
    "        ##################################\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        pred = None \n",
    "\n",
    "        ######### YOUR CODE HERE #########\n",
    "\n",
    "\n",
    "        ##################################\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def plot_decision_boundary(self, X, y):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', marker='o', s=30, edgecolors='k', label='Data Points')\n",
    "        plt.scatter(self.support_vectors[:, 0], self.support_vectors[:, 1], s=100, linewidth=1, facecolors='none',\n",
    "                    edgecolors='k', label='Support Vectors')\n",
    "\n",
    "        # Plot decision boundary\n",
    "        x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "        grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "        Z = self.predict(grid).reshape(xx.shape)\n",
    "\n",
    "        plt.contourf(xx, yy, Z, alpha=0.3, cmap='bwr')\n",
    "        plt.xlim(x_min, x_max)\n",
    "        plt.ylim(y_min, y_max)\n",
    "        plt.legend()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to generate overlapping linear data\n",
    "def generate_challenging_linear_data():\n",
    "    # Generate a mostly separable dataset\n",
    "    X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, \n",
    "                               n_clusters_per_class=1, class_sep=1.5, random_state=42)\n",
    "    y = np.where(y == 0, -1, 1)\n",
    "\n",
    "    # Introduce some overlap by adding noise\n",
    "    np.random.seed(42)\n",
    "    n_overlap = 5  # number of overlapping points\n",
    "    overlap_indices = np.random.choice(range(50, 100), n_overlap, replace=False)\n",
    "    \n",
    "    # Flip the labels of the overlapping points to create misclassifications\n",
    "    y[overlap_indices] = -y[overlap_indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate the overlapping linear data\n",
    "X_challenging, y_challenging = generate_challenging_linear_data()\n",
    "\n",
    "# Plot the data to visualize the overlap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_challenging[:, 0], X_challenging[:, 1], c=y_challenging, cmap='bwr', marker='o', s=50, edgecolors='k')\n",
    "plt.title(\"Challenging Linear Data with Overlap\")\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.show()\n",
    "\n",
    "dict_linear = {'kernel_type': 'linear'}\n",
    "dict_gaussian = {'kernel_type': 'gaussian', 'gamma': 1}\n",
    "dict_polynomial = {'kernel_type': 'polynomial', 'c': 1, 'degree': 3}\n",
    "# Try to fit a Hard Margin SVM (will likely fail)\n",
    "try:\n",
    "    \n",
    "    hard_margin_svm = HardMarginSVM(dict_gaussian)\n",
    "    hard_margin_svm.fit(X_challenging, y_challenging)\n",
    "    print(\"Hard Margin SVM results:\")\n",
    "    hard_margin_svm.plot_decision_boundary(X_challenging, y_challenging)\n",
    "except Exception as e:\n",
    "    print(\"Hard Margin SVM failed due to non-separable data:\", e)\n",
    "\n",
    "\n",
    "dict_linear = {'kernel_type': 'linear', 'C': 0.5}\n",
    "dict_gaussian = {'kernel_type': 'gaussian', 'gamma': 1, 'C': 1}\n",
    "dict_polynomial = {'kernel_type': 'polynomial', 'c': 1, 'degree': 3, 'C': 1}\n",
    "# Fit a Soft Margin SVM (should succeed with overlapping data)\n",
    "soft_margin_svm = SoftMarginSVM(dict_linear)\n",
    "soft_margin_svm.fit(X_challenging, y_challenging)\n",
    "print(\"Soft Margin SVM results:\")\n",
    "soft_margin_svm.plot_decision_boundary(X_challenging, y_challenging)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
