{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('moons_data.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\"coolwarm\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Moons data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, criterion=\"gini\", percent_of_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "        self.percent_of_features = percent_of_features\n",
    "\n",
    "    def fit(self, X, y, w):\n",
    "        self.tree = self.grow_tree(X, y, w)\n",
    "\n",
    "    def grow_tree(self, X, y, w, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(set(y))\n",
    "\n",
    "        # regularization - stop if max_depth is reached or if there is only one label\n",
    "        if (depth >= self.max_depth or num_labels == 1):\n",
    "            class_weights = {}\n",
    "            for c in np.unique(y):\n",
    "                class_weights[c] = np.sum(w[y == c])\n",
    "            majority_class = max(class_weights, key=class_weights.get)\n",
    "            return {\"label\": majority_class}\n",
    "        \n",
    "        number_of_features = int(num_features * self.percent_of_features)\n",
    "        features_index_to_consider = np.random.choice(num_features, number_of_features, replace=False)\n",
    "        best_split = self.best_split(X, y, w, features_index_to_consider)\n",
    "\n",
    "        if best_split[\"gain\"] == 0:\n",
    "            class_weights = {}\n",
    "            for c in np.unique(y):\n",
    "                class_weights[c] = np.sum(w[y == c])\n",
    "            majority_class = max(class_weights, key=class_weights.get)\n",
    "            return {\"label\": majority_class}\n",
    "\n",
    "        left = self.grow_tree(\n",
    "            best_split[\"X_left\"],\n",
    "            best_split[\"y_left\"],\n",
    "            best_split[\"w_left\"],\n",
    "            depth + 1,\n",
    "        )\n",
    "        right = self.grow_tree(\n",
    "            best_split[\"X_right\"],\n",
    "            best_split[\"y_right\"],\n",
    "            best_split[\"w_right\"],\n",
    "            depth + 1,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"feature\": best_split[\"feature\"],\n",
    "            \"threshold\": best_split[\"threshold\"],\n",
    "            \"left\": left,\n",
    "            \"right\": right,\n",
    "        }\n",
    "\n",
    "    # Find the best split for the current node in the tree with respect to one feature\n",
    "    # input: X, y, num_features\n",
    "    # output: split -> dictionary containing the feature, threshold, gain, X_left, y_left, X_right, y_right\n",
    "    def best_split(self, X, y, w, features_index_to_consider):\n",
    "        best_gain = -1\n",
    "        split = {\n",
    "            \"feature\": None,\n",
    "            \"threshold\": None,\n",
    "            \"gain\": None,\n",
    "            \"X_left\": None,\n",
    "            \"y_left\": None,\n",
    "            \"w_left\": None,\n",
    "            \"X_right\": None,\n",
    "            \"y_right\": None,\n",
    "            \"w_right\": None,\n",
    "        }\n",
    "        \n",
    "        ########## Your code goes here ##########\n",
    "        \n",
    "\n",
    "        ###########################################\n",
    "        \n",
    "\n",
    "        return split\n",
    "\n",
    "    # split the data into two regions based on the feature and threshold and computes the information gain\n",
    "    def split(self, X, y, w, feature, threshold):\n",
    "        left_idx = X[:, feature] <= threshold\n",
    "        right_idx = X[:, feature] > threshold\n",
    "        X_left, y_left, w_left = X[left_idx], y[left_idx], w[left_idx]\n",
    "        X_right, y_right, w_right = X[right_idx], y[right_idx], w[right_idx]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            return 0, X_left, y_left, w_left, X_right, y_right, w_right\n",
    "\n",
    "        gain = self.information_gain(y, w, y_left, w_left, y_right, w_right)\n",
    "\n",
    "        return gain, X_left, y_left, w_left, X_right, y_right, w_right\n",
    "\n",
    "    def information_gain(self, y, w, y_left, w_left, y_right, w_right):\n",
    "        total_weight = np.sum(w)\n",
    "        weight_left = np.sum(w_left)\n",
    "        weight_right = np.sum(w_right)\n",
    "\n",
    "        if self.criterion == \"gini\":\n",
    "            gain = self.gini(y, w) - (\n",
    "                (weight_left / total_weight) * self.gini(y_left, w_left)\n",
    "                + (weight_right / total_weight) * self.gini(y_right, w_right)\n",
    "            )\n",
    "        else:\n",
    "            gain = self.entropy(y, w) - (\n",
    "                (weight_left / total_weight) * self.entropy(y_left, w_left)\n",
    "                + (weight_right / total_weight) * self.entropy(y_right, w_right)\n",
    "            )\n",
    "\n",
    "        return gain\n",
    "\n",
    "    # Compute the gini impurity\n",
    "    def gini(self, y, w):\n",
    "\n",
    "        gini_impurity = None\n",
    "\n",
    "        # The class probabilities should be calculated as the sum of the sample weights for each class normalized by the total sum of all weights\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        ###########################################\n",
    "\n",
    "        return gini_impurity\n",
    "\n",
    "    # Compute the entropy\n",
    "    def entropy(self, y, w):\n",
    "\n",
    "        entropy_value = None\n",
    "\n",
    "        # The class probabilities should be calculated as the sum of the sample weights for each class normalized by the total sum of all weights\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "        return entropy_value\n",
    "    \n",
    "    # Predict for a vector of inputs\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(inputs, self.tree) for inputs in X])\n",
    "\n",
    "    # Predict for a single input\n",
    "    def predict_single(self, inputs, tree):\n",
    "        if \"label\" in tree:\n",
    "            return tree[\"label\"]\n",
    "        feature = tree[\"feature\"]\n",
    "        threshold = tree[\"threshold\"]\n",
    "        if inputs[feature] <= threshold:\n",
    "            return self.predict_single(inputs, tree[\"left\"])\n",
    "        else:\n",
    "            return self.predict_single(inputs, tree[\"right\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost():\n",
    "\n",
    "    def __init__(self, n_estimators=50):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.alphas = []  # To store alpha values\n",
    "        self.models = []  # To store weak classifiers\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, _ = X.shape\n",
    "\n",
    "        # Initialize weights equally\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        \n",
    "        # Convert y to {+1, -1} for compatibility. Hint: Use np.where\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "\n",
    "            # Implement the Adaboost algorithm using Decision Stumps as weak learners\n",
    "            \n",
    "            ########## Your code goes here ##########\n",
    "\n",
    "            # Create a weak learner\n",
    "            \n",
    "            # Calculate error rate\n",
    "        \n",
    "            # Calculate alpha (importance of the weak learner)\n",
    "            \n",
    "            # Update weights\n",
    "\n",
    "            # Normalize weights\n",
    "\n",
    "            #########################################\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Compute the weighted sum of weak classifiers\n",
    "\n",
    "        pred = None\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        #########################################\n",
    "        return pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_boost = AdaBoost(n_estimators=35)\n",
    "ada_boost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(tree, X, y, title=\"\"):\n",
    "    # Define bounds of the plot\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    \n",
    "    # Create a grid of points with a small step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    # Use the classifier to predict the class at each grid point\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = tree.predict(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the contours\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=20, cmap=\"coolwarm\")\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundaries(ada_boost, X, y, title=\"Decision Boundary of the AdaBoost\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
