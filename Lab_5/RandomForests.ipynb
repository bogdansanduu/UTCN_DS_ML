{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('linear_separable_data.npz')\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=\"coolwarm\")\n",
    "plt.xlabel(\"X1\")\n",
    "plt.ylabel(\"X2\")\n",
    "plt.title(\"Linear separable data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=None, criterion=\"gini\", percent_of_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.criterion = criterion\n",
    "        self.tree = None\n",
    "        self.percent_of_features = percent_of_features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.tree = self.grow_tree(X, y)\n",
    "\n",
    "    def grow_tree(self, X, y, depth=0):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_labels = len(set(y))\n",
    "\n",
    "        # regularization - stop if max_depth is reached or if there is only one label\n",
    "        if (depth >= self.max_depth or num_labels == 1):\n",
    "            return {\"label\": Counter(y).most_common(1)[0][0]}\n",
    "        \n",
    "        number_of_features = int(num_features * self.percent_of_features)\n",
    "        features_index_to_consider = np.random.choice(num_features, number_of_features, replace=False)\n",
    "        best_split = self.best_split(X, y, features_index_to_consider)\n",
    "\n",
    "        if best_split[\"gain\"] == 0:\n",
    "            return {\"label\": Counter(y).most_common(1)[0][0]}\n",
    "\n",
    "        left = self.grow_tree(best_split[\"X_left\"], best_split[\"y_left\"], depth + 1)\n",
    "        right = self.grow_tree(best_split[\"X_right\"], best_split[\"y_right\"], depth + 1)\n",
    "\n",
    "        return {\n",
    "            \"feature\": best_split[\"feature\"],\n",
    "            \"threshold\": best_split[\"threshold\"],\n",
    "            \"left\": left,\n",
    "            \"right\": right,\n",
    "        }\n",
    "\n",
    "    # Find the best split for the current node in the tree with respect to one feature\n",
    "    # input: X, y, num_features\n",
    "    # output: split -> dictionary containing the feature, threshold, gain, X_left, y_left, X_right, y_right\n",
    "    def best_split(self, X, y, features_index_to_consider):\n",
    "\n",
    "        best_gain = -1\n",
    "        feature, threshold, gain, X_left, X_right, y_left, y_right = None, None, None , None, None, None, None\n",
    "        split = {\n",
    "            \"feature\": feature,\n",
    "            \"threshold\": threshold,\n",
    "            \"gain\": gain,\n",
    "            \"X_left\": X_left,\n",
    "            \"y_left\": y_left,\n",
    "            \"X_right\": X_right,\n",
    "            \"y_right\": y_right,\n",
    "        }\n",
    "        \n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "        return split\n",
    "\n",
    "    # split the data into two regions based on the feature and threshold and computes the information gain\n",
    "    def split(self, X, y, feature, threshold):\n",
    "        left_idx = X[:, feature] <= threshold\n",
    "        right_idx = X[:, feature] > threshold\n",
    "        X_left, y_left = X[left_idx], y[left_idx]\n",
    "        X_right, y_right = X[right_idx], y[right_idx]\n",
    "\n",
    "        if len(y_left) == 0 or len(y_right) == 0:\n",
    "            return 0, X_left, y_left, X_right, y_right\n",
    "\n",
    "        gain = self.information_gain(y, y_left, y_right)\n",
    "\n",
    "        return gain, X_left, y_left, X_right, y_right\n",
    "\n",
    "    # Compute the information gain\n",
    "    # Note that you must weight the left and right child nodes by the number of samples in each\n",
    "    def information_gain(self, y, y_left, y_right):\n",
    "\n",
    "        gain = None\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "        \n",
    "        ###########################################\n",
    "\n",
    "        return gain\n",
    "\n",
    "    # Compute the gini impurity\n",
    "    def gini(self, y):\n",
    "\n",
    "        gini_impurity = None\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "        return gini_impurity\n",
    "\n",
    "    # Compute the entropy\n",
    "    def entropy(self, y):\n",
    "\n",
    "        entropy_value = None\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "        return entropy_value\n",
    "    \n",
    "    # Predict for a vector of inputs\n",
    "    def predict(self, X):\n",
    "        return np.array([self.predict_single(inputs, self.tree) for inputs in X])\n",
    "\n",
    "    # Predict for a single input\n",
    "    def predict_single(self, inputs, tree):\n",
    "        if \"label\" in tree:\n",
    "            return tree[\"label\"]\n",
    "        feature = tree[\"feature\"]\n",
    "        threshold = tree[\"threshold\"]\n",
    "        if inputs[feature] <= threshold:\n",
    "            return self.predict_single(inputs, tree[\"left\"])\n",
    "        else:\n",
    "            return self.predict_single(inputs, tree[\"right\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Random Forest Algorithm\n",
    "\n",
    "In the previous lab, we implemented the Decision Tree classifier. In this lab, we will build on that implementation to develop and test the Random Forest algorithm.\n",
    "\n",
    "Before implementing the Random Forest, we need to add one more property to the Decision Tree, namely the percentage of features that will be sampled when determining the best split. This property is essential as described in the lab instructions.\n",
    "\n",
    "Make a copy of the previous Decision Tree implementation and ensure the necessary changes are made to accommodate the new class variable `percent_of_features`, which represents the percentage of the total number of features per example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_trees=80, max_depth=5, criterion=\"gini\", percent_of_features=None, percent_of_samples=None):\n",
    "        # Initialize the random forest\n",
    "        # input: n_trees -> number of trees in the forest\n",
    "        #        max_depth -> maximum depth of the tree\n",
    "        #        criterion -> criterion to use for splitting\n",
    "        #        percent_of_features -> percentage of features to consider for each split\n",
    "        #        percent_of_samples -> percentage of samples to consider for each tree\n",
    "\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = [DecisionTree(max_depth=max_depth, criterion=criterion, percent_of_features=percent_of_features) for i in range(n_trees)]\n",
    "        self.percent_of_samples = percent_of_samples\n",
    "\n",
    "    def split_data(self, X, y, n_trees):\n",
    "\n",
    "        # Split the data into n_trees number of samples\n",
    "        # input: X -> data\n",
    "        #        y -> labels\n",
    "        #        n_trees -> number of trees\n",
    "        # output: X_split -> list of data split into n_trees number of samples\n",
    "        #         y_split -> list of labels split into n_trees number of samples\n",
    "        # Note: Make sure the split is random and that after picking one sample, it can be picked again. Hint: np.random.choice\n",
    "\n",
    "        X_split = []\n",
    "        y_split = []\n",
    "        for _ in range(n_trees):\n",
    "            \n",
    "            ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "            ###########################################\n",
    "\n",
    "        return X_split, y_split\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit the random forest\n",
    "        # input: X -> data\n",
    "        #        y -> labels\n",
    "        # Note: Split the data and fit each tree in the forest\n",
    "\n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict the label for each sample in X\n",
    "        # input: X -> data\n",
    "        # output: pred -> list of labels\n",
    "            \n",
    "        pred = None\n",
    "        \n",
    "        ########## Your code goes here ##########\n",
    "\n",
    "\n",
    "        ###########################################\n",
    "\n",
    "        return pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForest(n_trees = 30, max_depth=5, criterion=\"gini\", percent_of_features=0.5, percent_of_samples=0.5)\n",
    "forest.fit(X, y)\n",
    "\n",
    "tree = DecisionTree(max_depth=3, criterion=\"gini\", percent_of_features=1.)\n",
    "tree.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(tree, X, y, title=\"\"):\n",
    "    # Define bounds of the plot\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Create a grid of points with a small step\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
    "                         np.arange(y_min, y_max, 0.01))\n",
    "    \n",
    "    # Use the classifier to predict the class at each grid point\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = tree.predict(grid)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the contours\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=\"coolwarm\")\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, s=10, cmap=\"coolwarm\")\n",
    "    plt.xlabel(\"X1\")\n",
    "    plt.ylabel(\"X2\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundaries(forest, X, y, title=\"Random Forest\")\n",
    "plot_decision_boundaries(tree, X, y, title=\"Decision Tree\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
